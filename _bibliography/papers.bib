---
---

@string{aps = {American Physical Society}}

@article{cheong2024legaladvice,
  bibtex_show={true},
  title={(A)I Am Not a Lawyer, But...: Engaging Legal Experts towards Responsible LLM Policies for Legal Advice},
  author={Cheong, Inyoung and Xia, King and Feng, K.J. Kevin and Chen, Quan Ze and Zhang, Amy X.},
  journal={Arxiv},
  year={2024},
  pdf={legaladvice.pdf},
  preview={legal.jpg},
  website={https://arxiv.org/abs/2402.01864},
  selected={true},
  abstract={The rapid proliferation of large language models (LLMs) as general purpose chatbots available to the public raises hopes around expanding access to professional guidance in law, medicine, and finance, while triggering concerns about public reliance on LLMs for high-stakes circumstances. Prior research has speculated on high-level ethical considerations but lacks concrete criteria determining when and why LLM chatbots should or should not provide professional assistance. Through examining the legal domain, we contribute a structured expert analysis to uncover nuanced policy considerations around using LLMs for professional advice, using methods inspired by case-based reasoning. We convened workshops with 20 legal experts and elicited dimensions on appropriate AI assistance for sample user queries (``cases''). We categorized our expert dimensions into: (1) user attributes, (2) query characteristics, (3) AI capabilities, and (4) impacts. Beyond known issues like hallucinations, experts revealed novel legal problems, including that users' conversations with LLMs are not protected by attorney-client confidentiality or bound to professional ethics that guard against conflicted counsel or poor quality advice. This accountability deficit led participants to advocate for AI systems to help users polish their legal questions and relevant facts, rather than recommend specific actions. More generally, we highlight the potential of case-based expert deliberation as a method of responsibly translating professional integrity and domain knowledge into design requirements to inform appropriate AI behavior when generating advice in professional domains.}

@article{cheong2023neuripsmp2,
  bibtex_show={true},
  title={Case Repositories: Towards Case-Based Reasoning for AI Alignment},
  author={Feng, K.J. Kevin and Chen, Quan Ze and Cheong, Inyoung and Xia, King and Zhang, Amy X.},
  journal={NeurIPS 2023 MP2 Workshop},
  year={2023},
  pdf={caserepositories.pdf},
  preview={aichat.jpg},
  website={https://arxiv.org/abs/2311.10934},
  selected={true},
  abstract={Case studies commonly form the pedagogical backbone in law, ethics, and many other domains that face complex and ambiguous societal questions informed by human values. Similar complexities and ambiguities arise when we consider how AI should be aligned in practice: when faced with vast quantities of diverse (and sometimes conflicting) values from different individuals and communities, with whose values is AI to align, and how should AI do so? We propose a complementary approach to constitutional AI alignment, grounded in ideas from case-based reasoning (CBR), that focuses on the construction of policies through judgments on a set of cases. We present a process to assemble such a case repository by: 1) gathering a set of ``seed'' cases -- questions one may ask an AI system -- in a particular domain, 2) eliciting domain-specific key dimensions for cases through workshops with domain experts, 3) using LLMs to generate variations of cases not seen in the wild, and 4) engaging with the public to judge and improve cases. We then discuss how such a case repository could assist in AI alignment, both through directly acting as precedents to ground acceptable behaviors, and as a medium for individuals and communities to engage in moral reasoning around AI.}
}

@article{cheong2023genlaw,
  abbr={GenLaw},
  bibtex_show={true},
  title={Envisioning Legal Mitigations for LLM-based Intentional and Unintentional Harms (Extended Abstract)},
  author={Cheong, Inyoung and Caliskan, Aylin and Kohno, Tadayoshi},
  journal={ICML 2023 Workshop on Generative AI and Law},
  year={2023},
  pdf={32.pdf},
  preview={generativeai.jpg},
  website={https://genlaw.github.io/CameraReady/32.pdf},
  selected={true},
  abstract={Large language models (LLMs) have the potential for significant benefits, but they also pose risks such as privacy infringement, discrimination propagation, and virtual abuse. By developing and examining “worst-case” scenarios that illustrate LLM-based harms, this paper identifies that U.S. law may not be adequate in addressing threats to fundamental human rights posed by LLMs. The shortcomings arise from the primary focus of U.S. laws on governmental intrusion rather than market injustices, the complexities of LLM-related harms, and the intangible nature of these harms. As Section 230 protections for online intermediaries may not extend to AI-generated content, LLM developers must demonstrate due diligence (alignment efforts) to defend themselves against potential claims. Moving forward, we should consider ex-ante safety regulations adapted to LLMs to give clearer guidelines to the fast-paced AI development. Innovative interpretations or amendments to the Bills of Rights may be necessary to prevent the perpetuation of bias and uphold socio-economic rights.}
}

@article{cheong2023kpla,
  bibtex_show={true},
  title={Legal Perspectives on AI Alignment, Evaluation, and Interpretability},
  author={Cheong, Inyoung},
  journal={Korea Association for Telecommunications Policies Conference 2023},
  year={2023},
  pdf={ailawkorean.pdf},
  preview={aikorea.jpg},
  selected={true},
  abstract={The rise of generative AI has raised concerns around algorithmic bias and discrimination, privacy invasion, and the proliferation of harmful content. Europe is responding proactively with efforts to regulate AI systems. However, the US remains cautious about overarching legislation in this fast-moving domain. America's stance stems from its distinct legal tradition emphasizing free speech and limiting government intervention. This libertarian ethos leaves emerging technological issues largely to private lawsuits between individuals and companies. The current US legal and technological landscape faces challenges in proactively governing the risks of generative AI systems. The core issue is that these technologies are complex and opaque with limited interpretability. Their development involves many parties, and their societal impacts emerge gradually through widespread diffusion. In this environment, an approach fixated on assigning legal blame in isolated incidents proves insufficient. It cannot adequately detect emerging harms nor provide systemic incentives guiding development towards safety.
While understandable given American values, this reactive stance seems ill-suited for AI's breakneck pace and societal consequences. More comprehensive oversight and guidance throughout the technology lifecycle may prove essential. This includes setting clear expectations for safety practices and having processes to continually re-evaluate policies as capabilities advance. Rather than just responding to harms, the law can proactively shape technology's trajectory if coupled with scientific insight. This highlights the need for multidisciplinary collaboration and creative governance amidst AI's dynamism.}} 


@article{cheong2023freedom,
  bibtex_show={true},
  title={Freedom of Algorithmic Expression},
  author={Cheong, Inyoung},
  year={2023},
  journal={University of Cincinnati Law Review},
  volume={91},
  pages={680--740},
  pdf={freedom.pdf},
  website={https://scholarship.law.uc.edu/uclr/vol91/iss3/2/},
  preview={freedom2.jpg},
  dimensions={true},
  selected={true},
  abstract={Can content moderation on social media be considered a form of speech? If so, would government regulation of content moderation violate the First Amendment? These are the main arguments of social media companies after Florida and Texas legislators attempted to restrict social media platforms’ authority to de-platform objectionable content.
This article examines whether social media companies’ arguments have valid legal grounds. To this end, the article proposes three elements to determine that algorithms classify as “speech:” (1) the algorithms are designed to communicate messages; (2) the relevant messages reflect cognitive or emotive ideas beyond mere operational matters; and (3) they represent the company’s standpoints. The application of these elements makes it clear that social media algorithms can be considered speech when algorithms are designed to express companies’ values, ethics, and identity (as they often are).
However, conceptualizing algorithms as speech does not automatically award a social media company a magic shield against state or federal regulation. It is true that social media platforms’ position is likely to be favored by the U.S. Supreme Court, which has increasingly taken an all-or-nothing approach whereby “all speech invokes strict scrutiny of government regulation.” Instead, this article argues for the restoration of the Court’s approach prior to the 1970s, when decisions emphasized considerations such as the democratic values of speech, the irreplaceability of forums, and the socioeconomic inequality of speakers and audiences.
Under the latter principles, social media companies’ market dominance and their harmful effect on juveniles or political polarization would justify legislative efforts to increase algorithmic transparency even if they restrict social media’s free speech. Therefore, most big tech companies’ algorithms can and should be regulated for legitimate government purposes.}
}


@article{cheong2021epic,
  title={Epic Games v. Apple on App Store Payment Systems in South Korea},
  author={Cheong, Inyoung},
  bibtex_show={true},
  journal={Wash. J. of L. Tech. & Arts},
  year={2021},
  month={Feb},
  preview={epic.jpg},
  website={https://wjlta.com/2021/12/05/epic-games-v-apple-on-app-store-payment-systems-in-south-korea/}
}

@book{cheong2022diveritas,
  bibtex_show={true},
  title={Distinction Between Private and Public Space (Korean)},
  author={Cheong, Inyoung},
  publisher={Korea University Divertas Press},
  year={2021},
  month={July},
  pdf={diveritas.pdf},
  preview={diveritas.jpg}
}

@article{cheong2022administrative,
  title={The U.S. Federal Administrative Law and Civil Penalties (Korean)},
  author={Cheong, Inyoung},
  bibtex_show={true},
  journal={Administrative Law Journal},
  volume={69}, 
  pages={71-133},
  publisher={Korea Administrative Law and Practice Association},
  year={2022},
  preview={civilpenalty.jpg},
  month={November},
  pdf={civil-penalty.pdf},
  abstract={This article aims to introduce the landscape of American civil penalties to Korean legal academia by analyzing U.S. federal statutes and case laws in areas such as antitrust, securities regulations, occupational safety, and environmental protection. The analysis highlights the growing authority of administrative agencies in imposing monetary penalties, replacing traditional court proceedings. American scholars have raised concerns about the constitutionality of administrative civil penalties. Issues include the blurred distinction between criminal and civil penalties, potentially violating the Double Jeopardy Clause and the Seventh Amendment's right to a jury trial. Moreover, current practices may encourage coerced settlements, where individuals accept administrative agency deals under the threat of criminal penalties. As a result, U.S. federal civil penalties have theoretical and practical flaws.
The article references the Jackesy v. SEC case, where the Fifth Circuit ruled that the SEC's civil penalties violated the Seventh Amendment due to the lack of a jury trial in administrative procedures. However, the article critiques the "public rights" theory in the Fifth Circuit's opinion, arguing that it lacks solid grounding in U.S. case law. Instead, the article suggests developing more effective judicial review systems for administrative penalties, acknowledging that courts may not be well-equipped to handle all disputes efficiently.
The evolution of U.S. federal civil penalties provides insights for South Korean penalty systems, reflecting the challenge of balancing conflicting values like adversarial legalism, due process, cost-efficient policy implementation, and administrative accountability.}
}
